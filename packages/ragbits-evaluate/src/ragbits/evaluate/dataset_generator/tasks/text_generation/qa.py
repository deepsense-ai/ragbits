from typing import Any
from distilabel.llms.base import LLM

from ragbits.core.prompt import Prompt
from ...prompts.qa import BasicAnswerGenPrompt, PassagesGenPrompt, QueryGenPrompt
from ...utils import get_closest_substring, get_passages_list


from .base import BaseDistilabelTask


class QueryGenTask(BaseDistilabelTask):
    """
    A task for generating a question based on a provided text chunk.
    """

    def __init__(self, llm: LLM, prompt_class: type[Prompt] = QueryGenPrompt, **kwargs):
        super().__init__(llm=llm, inputs=["chunk"], outputs=["question", "chunk"], prompt_class=prompt_class, **kwargs)

    def format_output(self, output: str, input: dict[str, Any] | None = None) -> dict[str, str]:
        """
        Formats the generated question into a structured dictionary with the original "chunk" input.

        Args:
            output: The generated question.
            input: Optional; contains "chunk" key with the original input chunk.

        Returns:
            A dictionary containing "chunk" and "question".
        """
        return {"chunk": input["chunk"], "question": output}


class PassagesGenTask(BaseDistilabelTask):
    """
    A task for generating passages related to a specific question and answer from a text chunk.
    """

    get_matches: bool = False

    def __init__(self, llm: LLM, prompt_class: type[Prompt] = PassagesGenPrompt, **kwargs):
        super().__init__(
            llm=llm,
            inputs=["chunk", "question", "basic_answer"],
            outputs=["question", "chunk", "passages"],
            prompt_class=prompt_class,
        )

    def format_output(self, output: str, input: dict[str, Any] | None = None) -> dict[str, list[str]]:
        """
        Formats the model's output into a structured dictionary with "question", "chunk", and "passages".
        If `get_matches` is `True`, attempts to find the closest matches for each passage within the
        provided chunk.

        Args:
            output: The raw output generated by the text generation model.
            input: Required if `get_matches` is `True`, containing "chunk"
                                           and "question".

        Returns:
            A dictionary with "chunk", "question", and a list of "passages".
        """
        passages = get_passages_list(output) or []

        if self.get_matches:
            matched_passages = []

            for passage in passages:
                if passage in input["chunk"]:
                    matched_passages.append(passage)
                else:
                    matched_passage = get_closest_substring(input["chunk"], passage)
                    matched_passages.append(matched_passage)

            return {"chunk": input["chunk"], "question": input["question"], "passages": matched_passages}

        return {"chunk": input["chunk"], "question": input["question"], "passages": passages}


class AnswerGenTask(BaseDistilabelTask):
    """
    A task for generating basic answers to questions based on a provided text chunk. This class extends
    the `TextGeneration` task from the `distilabel` package.
    """

    def __init__(self, llm: LLM, prompt_class: type[Prompt] = BasicAnswerGenPrompt, **kwargs):
        super().__init__(llm=llm, inputs=["chunk", "question"], outputs=["basic_answer"], prompt_class=prompt_class)

    def format_output(self, output: str, input: dict[str, Any] | None = None) -> dict[str, str]:
        """
        Formats the model's output into a structured dictionary with the "basic_answer" key.

        Args:
            output: The raw output generated by the text generation model.
            input: Optional; not typically used in this formatting.

        Returns:
            A dictionary with "basic_answer" as the key and the generated output as its value.
        """
        return {"basic_answer": output}
