import asyncio
from dataclasses import dataclass, field, replace
from typing import Any

import gradio as gr
import jinja2
from pydantic import BaseModel
from rich.console import Console

from ragbits.core.llms import LiteLLM
from ragbits.core.llms.clients import LiteLLMOptions
from ragbits.core.prompt import Prompt
from ragbits.dev_kit.prompt_lab.discovery.prompt_discovery import DEFAULT_FILE_PATTERN, PromptDiscovery


@dataclass(frozen=True)
class PromptState:
    """
    Class to store the current state of the application.

    This class holds various data structures used throughout the application's lifecycle.

    Attributes:
        prompts (list): A list containing discovered prompts.
        rendered_prompt (Prompt): The most recently rendered Prompt instance.
        llm_model_name (str): The name of the selected LLM model.
        llm_api_key (str | None): The API key for the chosen LLM model.
    """

    prompts: list = field(default_factory=list)
    rendered_prompt: Prompt | None = None
    llm_model_name: str | None = None
    llm_api_key: str | None = None


def render_prompt(index: int, system_prompt: str, user_prompt: str, state: gr.State, *args: Any) -> gr.State:
    """
    Renders a prompt based on the provided key, system prompt, user prompt, and input variables.

    This function constructs a Prompt object using the prompt constructor and input constructor
    associated with the given key. It then updates the current prompt in the application state.

    Args:
        index (int): The index of the prompt to render in the prompts state.
        system_prompt (str): The system prompt template for the prompt.
        user_prompt (str): The user prompt template for the prompt.
        state (PromptState): The application state object.
        args (tuple): A tuple of input values for the prompt.

    Returns:
        gr.State: The updated application state object.
    """
    prompt_class = state.prompts[index]
    prompt_class.system_prompt_template = jinja2.Template(system_prompt)
    prompt_class.user_prompt_template = jinja2.Template(user_prompt)

    input_type = prompt_class.input_type
    input_fields = get_input_type_fields(input_type)
    variables = {field["field_name"]: value for field, value in zip(input_fields, args)}
    input_data = input_type(**variables) if input_type is not None else None
    prompt_object = prompt_class(input_data=input_data)
    state = replace(state, rendered_prompt=prompt_object)

    return state


def list_prompt_choices(state: gr.State) -> list[tuple[str, int]]:
    """
    Returns a list of prompt choices based on the discovered prompts.

    This function generates a list of tuples containing the names of discovered prompts and their
    corresponding indices.

    Args:
        state (gr.State): The application state object.

    Returns:
        list[tuple[str, int]]: A list of tuples containing prompt names and their indices.
    """
    return [(prompt.__name__, idx) for idx, prompt in enumerate(state.value.prompts)]


def send_prompt_to_llm(state: gr.State) -> str:
    """
    Sends the current prompt to the LLM and returns the response.

    This function creates a LiteLLM client using the LLM model name and API key stored in the
    application state. It then calls the LLM client to generate a response based on the current prompt.

    Args:
        state (gr.State): The application state object.

    Returns:
        str: The response generated by the LLM.
    """
    llm_client = LiteLLM(model_name=state.llm_model_name, api_key=state.llm_api_key)

    try:
        response = asyncio.run(
            llm_client.client.call(conversation=state.rendered_prompt.chat, options=LiteLLMOptions())
        )
    except Exception as e:  # pylint: disable=broad-except
        response = str(e)

    return response


def get_input_type_fields(obj: BaseModel | None) -> list[dict]:
    """
    Retrieves the field names and default values from the input type of a prompt.

    This function inspects the input type object associated with a prompt and extracts information
    about its fields, including their names and default values.

    Args:
        obj (BaseModel): The input type object of the prompt.

    Returns:
        list[dict]: A list of dictionaries, each containing a field name and its default value.
    """
    if obj is None:
        return []
    return [
        {"field_name": k, "field_default_value": v["schema"].get("default", None)}
        for (k, v) in obj.__pydantic_core_schema__["schema"]["fields"].items()
    ]


def lab_app(  # pylint: disable=missing-param-doc
    file_pattern: str = DEFAULT_FILE_PATTERN, llm_model: str | None = None, llm_api_key: str | None = None
) -> None:
    """
    Launches the interactive application for listing, rendering, and testing prompts
    defined within the current project.
    """
    prompts = PromptDiscovery(file_pattern=file_pattern).discover()

    if not prompts:
        Console(stderr=True).print(
            f"""No prompts were found for the given file pattern: [b]{file_pattern}[/b].

Please make sure that you are executing the command from the correct directory \
or provide a custom file pattern using the [b]--file-pattern[/b] flag."""
        )
        return

    with gr.Blocks() as gr_app:
        prompts_state = gr.State(
            PromptState(
                llm_model_name=llm_model,
                llm_api_key=llm_api_key,
                prompts=list(prompts),
            )
        )

        prompt_selection_dropdown = gr.Dropdown(
            choices=list_prompt_choices(prompts_state), value=0, label="Select Prompt"
        )

        @gr.render(inputs=[prompt_selection_dropdown, prompts_state])
        def show_split(index: int, state: gr.State) -> None:
            prompt = state.prompts[index]
            list_of_vars = []
            with gr.Row():
                with gr.Column(scale=1):
                    with gr.Tab("Inputs"):
                        input_fields: list = get_input_type_fields(prompt.input_type)
                        for entry in input_fields:
                            with gr.Row():
                                var = gr.Textbox(
                                    label=entry["field_name"],
                                    value=entry["field_default_value"],
                                    interactive=True,
                                )
                                list_of_vars.append(var)

                        render_prompt_button = gr.Button(value="Render prompts")

                with gr.Column(scale=4):
                    with gr.Tab("Prompt"):
                        with gr.Row():
                            with gr.Column():
                                prompt_details_system_prompt = gr.Textbox(
                                    label="System Prompt", value=prompt.system_prompt, interactive=True
                                )

                            with gr.Column():
                                system_message = state.rendered_prompt.system_message if state.rendered_prompt else ""
                                gr.Textbox(label="Rendered System Prompt", value=system_message, interactive=False)

                        with gr.Row():
                            with gr.Column():
                                prompt_details_user_prompt = gr.Textbox(
                                    label="User Prompt", value=prompt.user_prompt, interactive=True
                                )

                            with gr.Column():
                                user_message = state.rendered_prompt.user_message if state.rendered_prompt else ""
                                gr.Textbox(label="Rendered User Prompt", value=user_message, interactive=False)

            llm_enabled = state.llm_model_name is not None
            prompt_ready = state.rendered_prompt is not None
            llm_request_button = gr.Button(
                value="Send to LLM",
                interactive=llm_enabled and prompt_ready,
            )
            gr.Markdown(
                "To enable this button, select an LLM model when starting the app in CLI.", visible=not llm_enabled
            )
            gr.Markdown("To enable this button, render a prompt first.", visible=llm_enabled and not prompt_ready)
            llm_prompt_response = gr.Textbox(lines=10, label="LLM response")

            render_prompt_button.click(
                render_prompt,
                [
                    prompt_selection_dropdown,
                    prompt_details_system_prompt,
                    prompt_details_user_prompt,
                    prompts_state,
                    *list_of_vars,
                ],
                [prompts_state],
            )
            llm_request_button.click(send_prompt_to_llm, prompts_state, llm_prompt_response)

    gr_app.launch()
